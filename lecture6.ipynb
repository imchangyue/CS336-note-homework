{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dfd6de4",
   "metadata": {},
   "source": [
    "## benchmarking and profilinng\n",
    "IMPORTANT: benchmark/profile your code!\n",
    "You can read spec sheets (marketing material) and papers\n",
    "...but performance depends on your library version, your hardware, your workload\n",
    "...so there is no substitute for benchmarking/profiling your code.\n",
    "\n",
    "Example computation: running forward/backward passes on an MLP.\n",
    "    run_mlp(dim=128, num_layers=16, batch_size=128, num_steps=5)\n",
    " - benchmarking()       # How long does it take?\n",
    " - profiling()          # Where time is being spent?\n",
    "    \n",
    "<mark>Every time you make a change, benchmark/profile!</mark>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df4e2b1",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## 性能基准测试与多层感知器 (MLP)\n",
    "\n",
    "### 什么是性能基准测试？\n",
    "\n",
    "**性能基准测试** (Benchmarking) 是一种衡量执行某项操作所需\\*\\*“挂钟时间” (wall-clock time)\\*\\* 的方法。它提供了端到端的时间消耗，但不会告诉你具体时间花在了哪里。要进行更细粒度的分析，需要使用性能剖析 (Profiling) 工具。\n",
    "\n",
    "性能基准测试在以下场景中非常有用：\n",
    "\n",
    "  * **比较不同实现**：找出哪种实现方式更快。\n",
    "  * **理解性能扩展**：了解性能如何随参数（如维度、批量大小）的变化而变化。\n",
    "\n",
    "-----\n",
    "\n",
    "### MLP 模型定义\n",
    "\n",
    "我们使用一个简单的多层感知器 (MLP) 模型进行测试。该模型由一系列线性层和 GeLU 激活函数交替组成。\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"一个简单的MLP：线性 -> GeLU -> 线性 -> GeLU -> ... -> 线性 -> GeLU\"\"\"\n",
    "    def __init__(self, dim: int, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(dim, dim) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            x = torch.nn.functional.gelu(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "### 基准测试函数\n",
    "\n",
    "`benchmark` 函数用于对任意可调用函数进行计时。为了获得更准确的结果，它包含了**预热 (warmup)** 阶段和多次试验 (`num_trials`)。预热阶段可以避免由于编译和缓存未命中导致的初始运行时间较慢。\n",
    "\n",
    "```python\n",
    "import time\n",
    "from typing import Callable\n",
    "from statistics import mean\n",
    "\n",
    "def benchmark(description: str, run: Callable, num_warmups: int = 1, num_trials: int = 3):\n",
    "    \"\"\"\n",
    "    通过运行 `num_trials` 次来对 `func` 进行基准测试，并返回所有时间。\n",
    "    - 预热：前几次运行可能会因为编译和缓存问题较慢，我们只关心稳定状态下的时间。\n",
    "    - 在CUDA可用时，会调用 torch.cuda.synchronize() 来等待所有CUDA线程完成，\n",
    "      这对于准确计时非常重要。\n",
    "    \"\"\"\n",
    "    # 预热阶段\n",
    "    for _ in range(num_warmups):\n",
    "        run()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # 实际计时\n",
    "    times: list[float] = []\n",
    "    for trial in range(num_trials):\n",
    "        start_time = time.time()\n",
    "        run()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "        times.append((end_time - start_time) * 1000) # 转换为毫秒\n",
    "\n",
    "    mean_time = mean(times)\n",
    "    return mean_time\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "### 性能基准测试结果分析\n",
    "\n",
    "我们对矩阵乘法和 MLP 模型在不同参数下的性能进行了基准测试。\n",
    "\n",
    "#### 1\\. 矩阵乘法\n",
    "\n",
    "我们测试了不同维度 (`dim`) 下的方阵乘法性能。\n",
    "\n",
    "| 维度 (dim) | 耗时 (ms) |\n",
    "| :--- | :--- |\n",
    "| 1024 | ... |\n",
    "| 2048 | ... |\n",
    "| 4096 | ... |\n",
    "| 8192 | ... |\n",
    "| 16384 | ... |\n",
    "\n",
    "**观察**：随着维度增加，矩阵乘法的计算量呈 $O(\\\\text{dim}^3)$ 增长，因此耗时会显著增加。\n",
    "\n",
    "#### 2\\. MLP 模型性能\n",
    "\n",
    "我们以一个基准配置作为参考：`dim=256, num_layers=4, batch_size=256, num_steps=2`。\n",
    "\n",
    "  * **基准 MLP 耗时**：`mlp_base` 记录了此配置下的平均耗时。\n",
    "\n",
    "-----\n",
    "\n",
    "##### 性能扩展性分析：\n",
    "\n",
    "我们分别增加 `num_steps`、`num_layers`、`batch_size` 和 `dim` 来观察性能变化。\n",
    "\n",
    "**扩展 `num_steps`**：\n",
    "`num_steps` 是循环运行模型的次数。\n",
    "\n",
    "  * **结果**：`run_mlp` 的耗时与 `num_steps` 呈**线性关系**。\n",
    "\n",
    "| 倍数 (scale) | num\\_steps | 耗时 (ms) |\n",
    "| :--- | :--- | :--- |\n",
    "| 2 | 4 | ... |\n",
    "| 3 | 6 | ... |\n",
    "| 4 | 8 | ... |\n",
    "| 5 | 10 | ... |\n",
    "\n",
    "**扩展 `num_layers`**：\n",
    "`num_layers` 决定了 MLP 中线性层和 GeLU 层的数量。\n",
    "\n",
    "  * **结果**：耗时与 `num_layers` 呈**线性关系**，因为计算量是线性增加的。\n",
    "\n",
    "| 倍数 (scale) | num\\_layers | 耗时 (ms) |\n",
    "| :--- | :--- | :--- |\n",
    "| 2 | 8 | ... |\n",
    "| 3 | 12 | ... |\n",
    "| 4 | 16 | ... |\n",
    "| 5 | 20 | ... |\n",
    "\n",
    "**扩展 `batch_size`**：\n",
    "`batch_size` 决定了每次前向/反向传播中并行处理的样本数。\n",
    "\n",
    "  * **结果**：耗时与 `batch_size` 呈**线性关系**。\n",
    "\n",
    "| 倍数 (scale) | batch\\_size | 耗时 (ms) |\n",
    "| :--- | :--- | :--- |\n",
    "| 2 | 512 | ... |\n",
    "| 3 | 768 | ... |\n",
    "| 4 | 1024 | ... |\n",
    "| 5 | 1280 | ... |\n",
    "\n",
    "**扩展 `dim`**：\n",
    "`dim` 是模型的输入/输出维度，也是线性层中矩阵乘法的维度。\n",
    "\n",
    "  * 线性层中的矩阵乘法计算量为 $O(\\\\text{dim}^2)$。\n",
    "  * **结果**：耗时与 `dim` 呈**平方关系**（或接近），这比其他参数扩展的影响更显著。\n",
    "\n",
    "| 倍数 (scale) | dim | 耗时 (ms) |\n",
    "| :--- | :--- | :--- |\n",
    "| 2 | 512 | ... |\n",
    "| 3 | 768 | ... |\n",
    "| 4 | 1024 | ... |\n",
    "| 5 | 1280 | ... |\n",
    "\n",
    "-----\n",
    "\n",
    "### 注意事项\n",
    "\n",
    "  * 测试结果可能会因 CUDA 内核、硬件等因素而有所波动，因此不总是完全可预测的。\n",
    "  * `torch.utils.benchmark` 是一个功能更强大的基准测试工具，提供了更多便利。这里为了让过程更透明，我们使用了自定义的简单实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210ffc18",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
