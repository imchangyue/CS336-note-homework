============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-8.4.1, pluggy-1.6.0
rootdir: /home/code/cs336/assignment1
configfile: pyproject.toml
plugins: jaxtyping-0.3.2
collected 3 items

tests/test_train_bpe.py::test_train_bpe_speed 5.871927499771118
FAILED
tests/test_train_bpe.py::test_train_bpe FAILED
tests/test_train_bpe.py::test_train_bpe_special_tokens FAILED

=================================== FAILURES ===================================
_____________________________ test_train_bpe_speed _____________________________

    def test_train_bpe_speed():
        """
        Ensure that BPE training is relatively efficient by measuring training
        time on this small dataset and throwing an error if it takes more than 1.5 seconds.
        This is a pretty generous upper-bound, it takes 0.38 seconds with the
        reference implementation on my laptop. In contrast, the toy implementation
        takes around 3 seconds.
        """
        input_path = FIXTURES_PATH / "corpus.en"
        start_time = time.time()
        _, _ = run_train_bpe(
            input_path=input_path,
            vocab_size=500,
            special_tokens=["<|endoftext|>"]
        )
        end_time = time.time()
        print(end_time - start_time)
>       assert end_time - start_time < 1.5
E       assert (1753947031.18449 - 1753947025.3125625) < 1.5

tests/test_train_bpe.py:25: AssertionError
________________________________ test_train_bpe ________________________________

    def test_train_bpe():
        input_path = FIXTURES_PATH / "corpus.en"
        vocab, merges = run_train_bpe(
            input_path=input_path,
            vocab_size=500,
            special_tokens=["<|endoftext|>"],
        )
    
        # Path to the reference tokenizer vocab and merges
        reference_vocab_path = FIXTURES_PATH / "train-bpe-reference-vocab.json"
        reference_merges_path = FIXTURES_PATH / "train-bpe-reference-merges.txt"
    
        # Compare the learned merges to the expected output merges
        gpt2_byte_decoder = {v: k for k, v in gpt2_bytes_to_unicode().items()}
        with open(reference_merges_path, encoding="utf-8") as f:
            gpt2_reference_merges = [tuple(line.rstrip().split(" ")) for line in f]
            reference_merges = [
                (
                    bytes([gpt2_byte_decoder[token] for token in merge_token_1]),
                    bytes([gpt2_byte_decoder[token] for token in merge_token_2]),
                )
                for merge_token_1, merge_token_2 in gpt2_reference_merges
            ]
    
    
>       assert merges == reference_merges
E       AssertionError: assert [(b' ', b't')...', b't'), ...] == [(b' ', b't')...', b'e'), ...]
E         
E         At index 1 diff: (b' ', b't') != (b' ', b'a')
E         
E         Full diff:
E           [
E               (
E                   b' ',...
E         
E         ...Full output truncated (1939 lines hidden), use '-vv' to show

tests/test_train_bpe.py:53: AssertionError
________________________ test_train_bpe_special_tokens _________________________

snapshot = <tests.conftest.Snapshot object at 0x7fcec6c8b980>

    def test_train_bpe_special_tokens(snapshot):
        """
        Ensure that the special tokens are added to the vocabulary and not
        merged with other tokens.
        """
        input_path = FIXTURES_PATH / "tinystories_sample_5M.txt"
        vocab, merges = run_train_bpe(
            input_path=input_path,
            vocab_size=1000,
            special_tokens=["<|endoftext|>"],
        )
    
        # Check that the special token is not in the vocab
        vocabs_without_specials = [word for word in vocab.values() if word != b"<|endoftext|>"]
        for word_bytes in vocabs_without_specials:
            assert b"<|" not in word_bytes
>       snapshot.assert_match(
            {
                "vocab_keys": set(vocab.keys()),
                "merges": merges,
                "vocab_values": set(vocab.values()),
            },
        )

tests/test_train_bpe.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.conftest.Snapshot object at 0x7fcec6c8b980>
actual = {'merges': [(b'h', b'e'), (b'h', b'e'), (b'h', b'e'), (b'h', b'e'), (b'h', b'e'), (b'h', b'e'), ...], 'vocab_keys': {0, 1, 2, 3, 4, 5, ...}, 'vocab_values': {b'\x00', b'\x01', b'\x02', b'\x03', b'\x04', b'\x05', ...}}
test_name = 'test_train_bpe_special_tokens', force_update = False

>   ???
E   AssertionError: Data for key 'merges' does not match snapshot for test_train_bpe_special_tokens
E   assert [(b'h', b'e')...', b'e'), ...] == [(b'h', b'e')...', b'd'), ...]
E     
E     At index 1 diff: (b'h', b'e') != (b' ', b't')
E     
E     Full diff:
E       [
E           (
E               b'h',...
E     
E     ...Full output truncated (5951 lines hidden), use '-vv' to show

D:\code\cs336\assignment1\tests\conftest.py:146: AssertionError
=============================== warnings summary ===============================
tests/adapters.py:291
  /home/code/cs336/assignment1/tests/adapters.py:291: SyntaxWarning: invalid escape sequence '\T'
    """Given the weights of a Transformer language model and input indices,

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/test_train_bpe.py::test_train_bpe_speed - assert (1753947031.184...
FAILED tests/test_train_bpe.py::test_train_bpe - AssertionError: assert [(b' ...
FAILED tests/test_train_bpe.py::test_train_bpe_special_tokens - AssertionErro...
=================== 3 failed, 1 warning in 183.59s (0:03:03) ===================
