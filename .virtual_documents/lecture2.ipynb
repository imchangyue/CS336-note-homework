


# 创建tensor的几种方法
import torch
x = torch.tensor([[1., 2, 3], [4, 5, 6]])  # @inspect x
x = torch.zeros(4, 8)  # 4x8 matrix of all zeros @inspect x
x = torch.ones(4, 8)  # 4x8 matrix of all ones @inspect x
x = torch.randn(4, 8)  # 4x8 matrix of iid Normal(0, 1) samples @inspect x

















import torch

# 直接使用 `torch.Tensor` 进行类型注解
x: torch.Tensor = torch.ones(2, 3, 4)  # 形状: (batch, seq1, hidden)
y: torch.Tensor = torch.ones(2, 3, 4)  # 形状: (batch, seq2, hidden)

# 传统方式：矩阵乘法
z = x @ y.transpose(-2, -1)  # batch, sequence, sequence

# 新方式：使用 einsum 进行操作
from einops import einsum
z_einsum = einsum(x, y, "batch seq1 hidden, batch seq2 hidden -> batch seq1 seq2")

# 打印结果
print(f"Traditional z: {z.shape}")
print(f"Einops z_einsum: {z_einsum.shape}")



import torch
from einops import reduce

# Define a tensor of shape (batch, seq, hidden)
x = torch.ones(2, 3, 4)  # Shape: (2, 3, 4)

# Old way to reduce tensor (e.g., mean along last dimension):
y = x.mean(dim=-1)  # Shape: (2, 3), reduced along the last dimension

# New (einops) way to reduce tensor (e.g., sum along the last dimension):
y = reduce(x, "... hidden -> ...", "sum")  # Shape: (2, 3), sum along the last dimension
y


import torch
from einops import rearrange, einsum

# Define a tensor of shape (batch, seq, total_hidden)
x = torch.ones(2, 3, 8)  # Shape: (2, 3, 8), where total_hidden = heads * hidden1


# Rearrange the total_hidden dimension into two dimensions (heads, hidden1)
x = rearrange(x, "... (heads hidden1) -> ... heads hidden1", heads=2)
# After this, x shape becomes (2, 3, 2, 4), where heads=2 and hidden1=4
print(x.size())


# Combine the heads and hidden2 dimensions back together
x = rearrange(x, "... heads hidden2 -> ... (heads hidden2)")
# After this, x shape becomes (2, 3, 8), heads and hidden2 are combined
print(x.size())





# 计算一个矩阵乘法所需要的FLOPS数量
B = 1024  # Number of points
D = 256  # Dimension
K = 64  # Number of outputs
x = torch.ones(B, D)
w = torch.randn(D, K)
y = x @ w
#We have one multiplication (x[i][j] * w[j][k]) and one addition per (i, j, k) triple.
actual_num_flops = 2 * B * D * K  # @inspect actual_num_flops
actual_num_flops

















x = torch.tensor([1., 2, 3])
w = torch.tensor([1., 1, 1], requires_grad=True)
pred_y = x @ w
loss = 0.5 * (pred_y - 5).pow(2)
print(loss)
# 进行反向传播
loss.backward()

# 确保 w.grad 被正确计算
assert torch.equal(w.grad, torch.tensor([1, 2, 3]))











# build a model
def custom_model():
    #Let's build up a simple deep linear model using nn.Parameter.
    D = 64  # Dimension
    num_layers = 2
    model = Cruncher(dim=D, num_layers=num_layers)
    param_sizes = [
        (name, param.numel())
        for name, param in model.state_dict().items()
    ]
    assert param_sizes == [
        ("layers.0.weight", D * D),
        ("layers.1.weight", D * D),
        ("final.weight", D),
    ]
    num_parameters = get_num_parameters(model)
    assert num_parameters == (D * D) + (D * D) + D
    #Remember to move the model to the GPU.
    device = get_device()
    model = model.to(device)
    #Run the model on some data.
    B = 8  # Batch size
    x = torch.randn(B, D, device=device)
    y = model(x)
    assert y.size() == torch.Size([B])


import torch.nn as nn
class Linear(nn.Module):
    """Simple linear layer."""
    def __init__(self, input_dim: int, output_dim: int):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(input_dim, output_dim) / np.sqrt(input_dim))
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return x @ self.weight


class Cruncher(nn.Module):
    def __init__(self, dim: int, num_layers: int):
        super().__init__()
        self.layers = nn.ModuleList([
            Linear(dim, dim)
            for i in range(num_layers)
        ])
        self.final = Linear(dim, 1)
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Apply linear layers
        B, D = x.size()
        for layer in self.layers:
            x = layer(x)
        # Apply final head
        x = self.final(x)
        assert x.size() == torch.Size([B, 1])
        # Remove the last dimension
        x = x.squeeze(-1)
        assert x.size() == torch.Size([B])
        return x





# Torch
seed = 0
torch.manual_seed(seed)
# NumPy
import numpy as np
np.random.seed(seed)
# Python
import random
random.seed(seed)








from collections.abc import Iterable  # 导入 Iterable
import torch
import torch.nn as nn

# 定义 AdaGrad 优化器
class AdaGrad(torch.optim.Optimizer):
    def __init__(self, params: Iterable[nn.Parameter], lr: float = 0.01):
        super(AdaGrad, self).__init__(params, dict(lr=lr))
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            for p in group["params"]:
                # Optimizer state
                state = self.state[p]
                grad = p.grad.data
                # Get squared gradients g2 = sum_{i<t} g_i^2
                g2 = state.get("g2", torch.zeros_like(grad))
                # Update optimizer state
                g2 += torch.square(grad)
                state["g2"] = g2
                # Update parameters
                p.data -= lr * grad / torch.sqrt(g2 + 1e-5)


# model memory
num_layers = 4
 # Parameters
num_parameters = (D * D * num_layers) + D  # @inspect num_parameters    assert num_parameters == get_num_parameters(model)
# Activations
num_activations = B * D * num_layers  # @inspect num_activations
#激活值（activations） 是指每一层的输出。它们是通过输入数据和网络参数进行运算得到的结果，通常是通过激活函数（例如 ReLU、Sigmoid 等）应用于加权和的结果。
# Gradients
num_gradients = num_parameters  # @inspect num_gradients
# Optimizer states
num_optimizer_states = num_parameters  # @inspect num_optimizer_states
# Putting it all together, assuming float32
total_memory = 4 * (num_parameters + num_activations + num_gradients + num_optimizer_states)  # @inspect total_memory





# 训练的基本流程复习
def train_loop():
    #Generate data from linear function with weights (0, 1, 2, ..., D-1).
    D = 16
    true_w = torch.arange(D, dtype=torch.float32, device=get_device())
    def get_batch(B: int) -> tuple[torch.Tensor, torch.Tensor]:
        x = torch.randn(B, D).to(get_device())
        true_y = x @ true_w
        return (x, true_y)
    #Let's do a basic run
    train("simple", get_batch, D=D, num_layers=0, B=4, num_train_steps=10, lr=0.01)
    #Do some hyperparameter tuning
    train("simple", get_batch, D=D, num_layers=0, B=4, num_train_steps=10, lr=0.1)
def train(name: str, get_batch,
          D: int, num_layers: int,
          B: int, num_train_steps: int, lr: float):
    model = Cruncher(dim=D, num_layers=0).to(get_device())
    optimizer = SGD(model.parameters(), lr=0.01)
    for t in range(num_train_steps):
        # Get data
        x, y = get_batch(B=B)
        # Forward (compute loss)
        pred_y = model(x)
        loss = F.mse_loss(pred_y, y)
        # Backward (compute gradients)
        loss.backward()
        # Update parameters
        optimizer.step()
        optimizer.zero_grad(set_to_none=True)






def checkpointing():
    #Training language models take a long time and certainly will certainly crash.
    #You don't want to lose all your progress.
    #During training, it is useful to periodically save your model and optimizer state to disk.
    model = Cruncher(dim=64, num_layers=3).to(get_device())
    optimizer = AdaGrad(model.parameters(), lr=0.01)
    #Save the checkpoint:
    checkpoint = {
        "model": model.state_dict(),
        "optimizer": optimizer.state_dict(),
    }
    torch.save(checkpoint, "model_checkpoint.pt")
    #Load the checkpoint:
    loaded_checkpoint = torch.load("model_checkpoint.pt")
