{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1b1c434",
   "metadata": {},
   "source": [
    "# scaling laws\n",
    "**核心思想**\n",
    "在过去，训练大型语言模型是一个繁琐且效率低下的过程。研究者们通常在大型模型上反复调整超参数（hyperparameters），比如学习率、批次大小等，来寻找最佳性能。这个过程非常耗时且成本高昂，正如幻灯片中提到的 \"Old and unpleasant\"（老旧且令人不快）。\n",
    "\n",
    "而新的方法（\"New (over?) optimism\"）则更加高效。研究发现，**大型语言模型的性能（如验证集损失）与其规模（如参数量、计算量、数据集大小）之间存在简单且可预测的数学关系**。这意味着：\n",
    "\n",
    " - 我们可以在小型模型上进行超参数调优，找到最佳配置。\n",
    " - 然后，我们可以利用这些缩放法则，将调优结果外推（extrapolate）到大型模型，从而预测大型模型在最佳配置下的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3039cb1d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
