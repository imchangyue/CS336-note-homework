### Problem (benchmarking_script): 4 points

![alt text](image.png)

(a)ä»£ç åœ¨cs336_systems/benchmark_profile.py
(b)åœ¨benchmark_results.csv
(c)åœ¨benchmark_results.csvé‡Œé¢ï¼Œæ ‡å‡†å·®å’Œå¹³å‡è€—æ—¶éƒ½å˜å¤§äº†

### Problem :
![alt text](image-1.png)
(a)ç”¨timeitè®°å½•çš„æ—¶é—´æ›´é•¿ä¸€ç‚¹ï¼Œç”¨nsysè®°å½•çš„æ”¾åœ¨resultæ–‡ä»¶å¤¹ä¸‹ï¼Œå…¶ä¸­xlå’Œ2.7Bçš„æ¨¡å‹éƒ½æœ‰ä¸€ç‚¹é—®é¢˜
(b)forwardæ—¶è°ƒç”¨æœ€å¤šçš„GPUå†…æ ¸æ˜¯ampere_bf16_s16816gemm_bf16_128x64_ldg8_relu_f2f_tn
![alt text](image-2.png)
ä½†æ˜¯åœ¨full trainingä¸­è°ƒç”¨æ—¶é—´æœ€é•¿çš„æ˜¯å¦ä¸€ä¸ª
![alt text](image-3.png)
(c)é€å…ƒç´ è¿ç®— (elementwise kernels) â†’ mulã€addã€masked fill ç­‰ (~20% ä»¥ä¸Šç´¯è®¡)ã€‚

Softmax å‰å‘ (softmax_warp_forward) (~4â€“5%)ã€‚

æ¿€æ´»å‡½æ•° (GELU) (~1%)ã€‚

LayerNorm å‰å‘ (~0.5%)ã€‚

(d)è®­ç»ƒæ­¥éª¤ä¸­çŸ©é˜µä¹˜æ³•çš„æ—¶é—´å æ¯”ä¼šæ˜¾è‘—é«˜äºæ¨ç†ã€‚è¿™æ˜¯å› ä¸ºåå‘ä¼ æ’­ï¼ˆbackward passï¼‰æœ¬è´¨ä¸Šå°±æ˜¯åˆ©ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æ¢¯åº¦ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¼šå¼•å…¥å¤§é‡çš„çŸ©é˜µä¹˜æ³•æ¥è®¡ç®—æƒé‡çš„æ¢¯åº¦ã€‚

(e)FLOPs å·®å¼‚ï¼š çŸ©é˜µä¹˜æ³• FLOPs >>> Softmax FLOPsã€‚

è¿è¡Œæ—¶é•¿å·®å¼‚ï¼š çŸ©é˜µä¹˜æ³•è¿è¡Œæ—¶é•¿å¯èƒ½åªç•¥é•¿äºæˆ–ä¸ Softmax ç›¸å½“ï¼Œè€Œä¸æ˜¯åƒ FLOPs é‚£æ ·å‘ˆæ•°é‡çº§å·®è·ã€‚


### problem
![alt text](image-4.png)
```bash
root@1ee5b610c063:/home/code_backup/code/cs336/assignment2-systems# python3 cs336_systems/mix_presicion.py
tensor(10.0001)
tensor(9.9531, dtype=torch.float16)
tensor(10.0021)
tensor(10.0021)
```


### problem 
![alt text](image-5.png)

(a)
â€¢ the model parameters within the autocast context : FP16
â€¢ the output of the first feed-forward layer (ToyModel.fc1),  FP16
â€¢ the output of layer norm (ToyModel.ln),  FP32
â€¢ the modelâ€™s predicted logits,  FP32
â€¢ the loss,  FP32
â€¢ and the modelâ€™s gradients?  FP32

 - autocastä¸Šä¸‹æ–‡ä¸­çš„æ¨¡å‹å‚æ•°: æ¨¡å‹å‚æ•°ï¼ˆå³self.fc1.weight, self.fc2.weightï¼‰çš„åŸå§‹æ•°æ®ç±»å‹ä¿æŒFP32ä¸å˜ã€‚autocastå¹¶ä¸ä¼šæ”¹å˜å­˜å‚¨åœ¨æ¨¡å‹ä¸­çš„å‚æ•°ç±»å‹ï¼Œè€Œæ˜¯åœ¨è¿›è¡Œè®¡ç®—æ—¶ï¼Œæ ¹æ®æ“ä½œç±»å‹å°†è¾“å…¥æˆ–æƒé‡è½¬æ¢ä¸ºFP16ã€‚

 - ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼ˆToyModel.fc1ï¼‰çš„è¾“å‡º: FP16ã€‚åƒå…¨è¿æ¥å±‚ï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰è¿™æ ·çš„è®¡ç®—å¯†é›†å‹æ“ä½œï¼Œä¼šè¢«autocastè‡ªåŠ¨è¯†åˆ«å¹¶è½¬æ¢ä¸ºFP16è¿›è¡Œè®¡ç®—ï¼Œä»¥è·å¾—æ›´é«˜çš„æ€§èƒ½ã€‚

 - å±‚å½’ä¸€åŒ–ï¼ˆToyModel.lnï¼‰çš„è¾“å‡º: FP32ã€‚**å±‚å½’ä¸€åŒ–ï¼ˆLayerNormï¼‰**é€šå¸¸éœ€è¦æ›´é«˜çš„ç²¾åº¦æ¥é¿å…æ•°å€¼ä¸ç¨³å®šï¼Œå› ä¸ºå®ƒæ¶‰åŠåˆ°å‡å€¼å’Œæ–¹å·®çš„è®¡ç®—ã€‚å› æ­¤ï¼Œautocasté€šå¸¸ä¼šå°†è¿™ç±»æ“ä½œä¿ç•™åœ¨FP32ä»¥ä¿è¯è®­ç»ƒçš„ç¨³å®šæ€§ã€‚

 - å‹çš„é¢„æµ‹logits: FP32ã€‚ç”±äºæœ€åä¸€ä¸ªå…¨è¿æ¥å±‚fc2çš„è¾“å…¥ï¼ˆæ¥è‡ªlnï¼‰æ˜¯FP32ï¼Œå¹¶ä¸”autocastå€¾å‘äºå°†ç´¯ç§¯å’Œå½’çº¦æ“ä½œä¿æŒåœ¨FP32ï¼Œå› æ­¤æœ€ç»ˆçš„è¾“å‡ºï¼ˆå³logitsï¼‰ä¹Ÿä¼šæ˜¯FP32ã€‚

 - æŸå¤±ï¼ˆlossï¼‰: FP32ã€‚æŸå¤±é€šå¸¸æ˜¯åœ¨FP32ä¸­è®¡ç®—çš„ï¼Œä»¥é¿å…ç´¯ç§¯çš„èˆå…¥è¯¯å·®ã€‚å³ä½¿å‰å‘ä¼ æ’­çš„éƒ¨åˆ†æ“ä½œåœ¨FP16ä¸­è¿›è¡Œï¼Œautocastä¹Ÿä¼šå°†ç»“æœè½¬å›FP32è¿›è¡ŒæŸå¤±è®¡ç®—ã€‚

 - æ¨¡å‹çš„æ¢¯åº¦: åœ¨æ ‡å‡†çš„æ··åˆç²¾åº¦è®­ç»ƒæµç¨‹ä¸­ï¼Œåå‘ä¼ æ’­çš„æ¢¯åº¦åœ¨è®¡ç®—è¿‡ç¨‹ä¸­ä¼šä¿æŒFP16ï¼Œä½†åœ¨æ¢¯åº¦æ›´æ–°å‰ï¼Œä¼šè½¬æ¢ä¸ºFP32ã€‚è¿™æ˜¯å› ä¸ºï¼š

 - ä¸ºäº†åŠ é€Ÿåå‘ä¼ æ’­ï¼Œæ¢¯åº¦è®¡ç®—é€šå¸¸åœ¨FP16ä¸‹è¿›è¡Œã€‚ä¸ºäº†é¿å…æ¢¯åº¦å€¼åœ¨FP16ä¸­å½’é›¶ï¼Œä¼šä½¿ç”¨æ¢¯åº¦ç¼©æ”¾ï¼ˆloss scalingï¼‰ã€‚æœ€ç»ˆï¼Œæ¢¯åº¦åœ¨æ›´æ–°æ¨¡å‹å‚æ•°æ—¶ä¼šè½¬æ¢ä¸ºFP32ï¼Œå› ä¸ºæ¨¡å‹å‚æ•°æœ¬èº«å°±æ˜¯FP32ï¼Œå¹¶ä¸”FP32çš„ç²¾åº¦æ›´é«˜ï¼Œæœ‰åŠ©äºå‚æ•°æ›´æ–°çš„ç¨³å®šã€‚




(b)å±‚å½’ä¸€åŒ–å¯¹æ··åˆç²¾åº¦éå¸¸æ•æ„Ÿï¼Œä¸»è¦åŸå› åœ¨äºå…¶è®¡ç®—è¿‡ç¨‹ä¸­çš„å‡å€¼ï¼ˆmeanï¼‰å’Œæ–¹å·®ï¼ˆvarianceï¼‰è®¡ç®—ã€‚è¿™äº›ç»Ÿè®¡é‡éœ€è¦å¯¹æ•´ä¸ªå±‚è¿›è¡Œç´¯åŠ å’Œæ±‚å’Œï¼Œè¿™ä¸ªè¿‡ç¨‹ä¸­éå¸¸å®¹æ˜“äº§ç”Ÿæ•°å€¼ä¸‹æº¢æˆ–ä¸Šæº¢ï¼Œå¯¼è‡´ç²¾åº¦æŸå¤±æˆ–NaNå€¼ã€‚

(c)å¯¹åº”çš„ä»£ç æ˜¯`assignment2-systems/cs336_systems/mixed_precision_benchmark.py`,è¿è¡Œè„šæœ¬æ˜¯`assignment2-systems/cs336_systems/run_mixed_precision_profile.sh`
è¿è¡Œç»“æœå­˜åœ¨`assignment2-systems/mixed_precision_results.csv`

è¡¨ç°è‰¯å¥½çš„æƒ…å†µï¼š
Small/Mediumæ¨¡å‹ + è¾ƒå¤§context lengthï¼š1.5-2.0xåŠ é€Ÿ
è®¡ç®—é‡å……è¶³ï¼Œèƒ½å……åˆ†åˆ©ç”¨BF16åŠ é€Ÿ

è¡¨ç°è¾ƒå·®çš„æƒ…å†µï¼š
XLæ¨¡å‹ + batch_size=1ï¼š0.04-0.97xï¼ˆå˜æ…¢ï¼‰
è®¡ç®—å¹¶è¡Œåº¦ä¸è¶³ï¼Œè½¬æ¢å¼€é”€å ä¸»å¯¼

æœ€ä¼˜é…ç½®ï¼š
Mediumæ¨¡å‹ï¼Œcontext length 256-512ï¼š~1.7-2.0xåŠ é€Ÿ
åœ¨è®¡ç®—é‡å’Œå†…å­˜ä½¿ç”¨ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ç‚¹

### problem 
![alt text](image-6.png)
(a)ç‰©å“çš„ç”µè„‘è¿è¡Œ2.7Bæ¨¡å‹ä¼šå´©æºƒï¼Œæ‰€ä»¥ç”¨äº†xlä»£æ›¿ï¼ˆæ··åˆç²¾åº¦ï¼‰ï¼Œæ¯è½®æµ‹é‡æ—¶éƒ½æœ‰5æ¬¡warm up

 - inference only:![alt text](image-8.png)
 - fulling training:![alt text](image-7.png)
å†…å­˜æ—¶é—´çº¿åˆ†æä¸åŒºåˆ†
ç¬¬ä¸€å¼ å›¾ï¼ˆçº¯å‰å‘ä¼ æ’­ï¼‰ï¼š

å†…å­˜å˜åŒ–æ¨¡å¼ï¼šéšç€å‰å‘ä¼ æ’­çš„è¿›è¡Œï¼Œæ¨¡å‹çš„æ¯ä¸€å±‚éƒ½ä¼šäº§ç”Ÿæ–°çš„æ¿€æ´»å€¼ï¼Œè¿™äº›æ¿€æ´»å€¼è¢«ä¿å­˜åœ¨æ˜¾å­˜ä¸­ï¼Œå¯¼è‡´å†…å­˜ä½¿ç”¨é‡å¹³ç¨³ä¸”æŒç»­åœ°å¢åŠ ï¼Œç›´åˆ°è¾¾åˆ°å³°å€¼ã€‚

å³°å€¼ä¸é‡Šæ”¾ï¼šå†…å­˜å³°å€¼å‡ºç°åœ¨å‰å‘ä¼ æ’­ç»“æŸæ—¶ã€‚æ­¤æ—¶ï¼Œæ¨¡å‹æ‰€æœ‰å±‚çš„æ¿€æ´»å€¼éƒ½å·²ç”Ÿæˆã€‚ç”±äºæ˜¯çº¯æ¨ç†æ¨¡å¼ï¼Œä¸éœ€è¦è®¡ç®—æ¢¯åº¦ï¼Œå› æ­¤åœ¨å‰å‘ä¼ æ’­ç»“æŸåï¼Œè¿™äº›æ¿€æ´»å€¼å¯ä»¥è¢«é€æ­¥æˆ–å…¨éƒ¨é‡Šæ”¾ï¼Œå¯¼è‡´å†…å­˜ä½¿ç”¨é‡ç¼“æ…¢ä¸‹é™ã€‚

ä¸»è¦å†…å­˜æ¥æºï¼šå³°å€¼ä¸»è¦ç”±æ¨¡å‹å‚æ•°å’Œå‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­äº§ç”Ÿçš„æ¿€æ´»å€¼ç»„æˆã€‚

ç¬¬äºŒå¼ å›¾ï¼ˆå®Œæ•´è®­ç»ƒæ­¥éª¤ï¼‰ï¼š

å†…å­˜å˜åŒ–æ¨¡å¼ï¼š

å‰å‘ä¼ æ’­ï¼šä¸ç¬¬ä¸€å¼ å›¾ç±»ä¼¼ï¼Œå†…å­˜ä½¿ç”¨é‡å¹³ç¨³å¢åŠ ã€‚ä½†ç”±äºéœ€è¦ä¿å­˜ç”¨äºåå‘ä¼ æ’­çš„ä¸­é—´æ¿€æ´»å€¼ï¼Œå†…å­˜å ç”¨é€šå¸¸ä¼šæ¯”çº¯æ¨ç†æ¨¡å¼æ›´é«˜ã€‚

åå‘ä¼ æ’­ï¼šåˆ°è¾¾å‰å‘ä¼ æ’­çš„å³°å€¼åï¼Œå¼€å§‹è®¡ç®—æ¢¯åº¦ã€‚åå‘ä¼ æ’­ä¼šä¸´æ—¶åˆ†é…æ–°çš„æ˜¾å­˜æ¥å­˜å‚¨æ¢¯åº¦ï¼Œè¿™ä½¿å¾—å†…å­˜ä½¿ç”¨é‡å‡ºç°ä¸€ä¸ªæ›´é™¡å³­ã€æ›´é«˜çš„å³°å€¼ã€‚

æ¢¯åº¦æ¸…é›¶ï¼šåœ¨åå‘ä¼ æ’­å’Œä¼˜åŒ–å™¨æ­¥éª¤ç»“æŸåï¼Œç”¨äºå­˜å‚¨æ¢¯åº¦å’Œéƒ¨åˆ†ä¸­é—´æ¿€æ´»å€¼çš„æ˜¾å­˜ä¼šè¢«å¤§é‡é‡Šæ”¾ï¼Œå› æ­¤å†…å­˜ä½¿ç”¨é‡ä¼šæ€¥å‰§ä¸‹é™ã€‚

ä¸»è¦å†…å­˜æ¥æºï¼šè¿™ä¸ªé˜¶æ®µçš„å³°å€¼ç”±æ¨¡å‹å‚æ•°ã€å‰å‘æ¿€æ´»å€¼å’Œæ¢¯åº¦å¼ é‡å…±åŒç»„æˆã€‚

å¦‚ä½•åŒºåˆ†ä¸¤ä¸ªè¿‡ç¨‹
é€šè¿‡è§‚å¯Ÿå†…å­˜æ—¶é—´çº¿ï¼Œä½ å¯ä»¥æ ¹æ®ä»¥ä¸‹ä¸¤ç‚¹æ¥åŒºåˆ†å‰å‘ä¼ æ’­å’Œå®Œæ•´çš„è®­ç»ƒè¿‡ç¨‹ï¼š

å†…å­˜å³°å€¼ï¼šå®Œæ•´è®­ç»ƒè¿‡ç¨‹ï¼ˆå‰å‘+åå‘ï¼‰çš„å†…å­˜å³°å€¼æ˜æ˜¾é«˜äºçº¯å‰å‘ä¼ æ’­ï¼Œå› ä¸ºåå‘ä¼ æ’­éœ€è¦é¢å¤–çš„æ˜¾å­˜æ¥å­˜å‚¨æ¢¯åº¦ã€‚

å†…å­˜é‡Šæ”¾æ¨¡å¼ï¼šçº¯å‰å‘ä¼ æ’­çš„å†…å­˜é‡Šæ”¾æ˜¯å¹³ç¼“çš„ï¼Œå› ä¸ºå®ƒä¸»è¦æ˜¯åœ¨æ¨ç†ç»“æŸåé‡Šæ”¾æ¿€æ´»å€¼ã€‚è€Œå®Œæ•´è®­ç»ƒè¿‡ç¨‹åœ¨åå‘ä¼ æ’­å®Œæˆåä¼šå‡ºç°ä¸€ä¸ªçªç„¶ä¸”å¤§å¹…åº¦çš„å†…å­˜ä¸‹é™ï¼Œè¿™å¯¹åº”ç€æ¢¯åº¦å’Œä¸­é—´å¼ é‡çš„é‡Šæ”¾ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸æ˜æ˜¾çš„æ ‡å¿—ã€‚

(b)å…·ä½“çš„pklä¿å­˜åœ¨`./memory_snapshot/fp32`ä¸­

| ä¸Šä¸‹æ–‡å¼ºåº¦ | å‰å‘ä¼ æ’­å³°å€¼æ˜¾å­˜ (Peak Memory in MB/GB) | å®Œæ•´è®­ç»ƒæ­¥éª¤å³°å€¼æ˜¾å­˜ (Peak Memory in MB/GB) |
| ----- | ------------------------------- | --------------------------------- |
| 128   | 6.3                             | 12.0                              |
| 256   | 7.1                             | 12.8                              |
| 512   | 9.2                             | memoryæŠ¥é”™                          |


(c)å…·ä½“çš„pklä¿å­˜åœ¨`./memory_snapshot/mixed_presicion`ä¸­,æ¯”float32å†…å­˜å³°å€¼å°å¾ˆå¤šï¼ˆçº¦0.5å€ï¼‰

| ä¸Šä¸‹æ–‡(context length) | å‰å‘ä¼ æ’­å³°å€¼æ˜¾å­˜ (Peak Memory in MB/GB) | å®Œæ•´è®­ç»ƒæ­¥éª¤å³°å€¼æ˜¾å­˜ (Peak Memory in MB/GB) |
| ----- | ------------------------------- | --------------------------------- |
| 128   | 3.1                             | 6.0                               |
| 256   | 3.6                             | 6.4                               |
| 512   | 4.6                             | 7.5                               |

(d)
æ®‹å·®æµä¸Šçš„æ¿€æ´»å¼ é‡é€šå¸¸çš„å½¢çŠ¶æ˜¯ (batch_size, block_size, embed_dim)ã€‚é¢˜ç›®è¦æ±‚è®¡ç®—å•ä¸ªå¼ é‡çš„å¤§å°ï¼Œæˆ‘ä»¬å¯ä»¥å‡è®¾ batch_size ä¸º 1ã€‚

 - è®¡ç®—å¼ é‡å…ƒç´ æ€»æ•°ï¼š

    å…ƒç´ æ€»æ•° = block_size Ã— embed_dim

    å…ƒç´ æ€»æ•° = 512 Ã— 2560 = 1,310,720

 - è®¡ç®—å­—èŠ‚æ•°ï¼ˆå•ç²¾åº¦ FP32ï¼‰ï¼š

    å•ç²¾åº¦æµ®ç‚¹æ•°ï¼ˆFP32ï¼‰å ç”¨ 4 ä¸ªå­—èŠ‚ã€‚

    æ€»å­—èŠ‚æ•° = å…ƒç´ æ€»æ•° Ã— 4

    æ€»å­—èŠ‚æ•° = 1,310,720 Ã— 4 = 5,242,880 å­—èŠ‚

 - è½¬æ¢ä¸º MBï¼š

    å°†å­—èŠ‚æ•°é™¤ä»¥ 1024 * 1024
    ï¼ˆå³ 1,048,576ï¼‰ã€‚

    å¤§å°ï¼ˆMBï¼‰ = 5,242,880 / 1,048,576 â‰ˆ 5.0 MB

(e)å¤§çš„åˆ†é…ä¸»è¦æ¥è‡ª MLP æ¨¡å—å†…éƒ¨çš„è®¡ç®—ï¼Œå³ï¼š

ä¸º çº¿æ€§å±‚ï¼ˆLinear Layerï¼‰ çš„çŸ©é˜µä¹˜æ³•ç»“æœåˆ†é…çš„å¼ é‡ (çº¦ 3.1 MiB)ã€‚

ä¸º GELU æ¿€æ´»å‡½æ•° çš„è¾“å‡ºåˆ†é…çš„å¼ é‡ (çº¦ 3.1 MiB)ã€‚

### problem
![alt text](image-9.png)
ä»£ç åœ¨`cs336_systems/pytorch_attention_profile.py`  
| d_model | seq_len | forward_time | backward_time | memory_usage_mb | status |
|--|---|---|---|---|-----|
| 16 | 256 | 0.000252 | 0.000672 | 19.125 | OK |
| 16 | 1024 | 0.001081 | 0.00314 | 51.75 | OK |
| 16 | 4096 | 0.015391 | 0.047806 | 542.25 | OK |
| 16 | 8192 | 0.061903 | 0.682544 | 2092.25 | OK |
| 16 | 16384 | N/A | N/A | N/A | OOM |
| 32 | 256 | 0.000083 | 0.000584 | 20.0 | OK |
| 32 | 1024 | 0.001023 | 0.003058 | 55.25 | OK |
| 32 | 4096 | 0.015733 | 0.038343 | 556.25 | OK |
| 32 | 8192 | 0.061436 | 0.597976 | 2120.25 | OK |
| 32 | 16384 | N/A | N/A | N/A | OOM |
| 64 | 256 | 0.000575 | 0.001372 | 21.75 | OK |
| 64 | 1024 | 0.00107 | 0.003126 | 62.25 | OK |
| 64 | 4096 | 0.015507 | 0.036756 | 584.25 | OK |
| 64 | 8192 | 0.06332 | 0.765987 | 2176.25 | OK |
| 64 | 16384 | N/A | N/A | N/A | OOM |
| 128 | 256 | 0.000743 | 0.001004 | 25.25 | OK |
| 128 | 1024 | 0.001243 | 0.003604 | 76.25 | OK |
| 128 | 4096 | 0.01764 | 0.050293 | 640.25 | OK |
| 128 | 8192 | 0.175377 | 1.10893 | 2288.25 | OK |
| 128 | 16384 | N/A | N/A | N/A | OOM |


æ˜¾å­˜ä¸è¶³å‘ç”Ÿåœ¨ d_model=16, seq_len=16384 çš„é…ç½®ã€‚

å¯¹äºè¿™ä¸ª OOM é…ç½®çš„å†…å­˜ä½¿ç”¨ä¼°ç®—:
  Q, K, V å¼ é‡æ‰€éœ€å†…å­˜: 24.00 MB

  æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µ (S) æ‰€éœ€å†…å­˜: 8192.00 MB

  æ³¨æ„åŠ›è¾“å‡º (A) æ‰€éœ€å†…å­˜: 8.00 MB

  æ€»å‰å‘ä¼ æ’­æ‰€éœ€æ˜¾å­˜: çº¦ 8224.00 MB

  æ€»åå‘ä¼ æ’­æ‰€éœ€æ˜¾å­˜ (é¢å¤–): çº¦ 8192.00 MB


 - å‡å°‘å†…å­˜æˆæœ¬çš„æ–¹æ³•ï¼š

     - åˆ‡ç‰‡ï¼ˆTilingï¼‰: å®ƒå°†è¾“å…¥ Qã€Kã€V åˆ‡æˆå°å—ï¼ˆtileï¼‰ï¼Œå¹¶é€å—è®¡ç®—æ³¨æ„åŠ›ã€‚æ¯æ¬¡åªå°†ä¸€ä¸ªtileåŠ è½½åˆ°GPUçš„å¿«é€ŸSRAMä¸­è¿›è¡Œè®¡ç®—ï¼Œç„¶åå°†ç»“æœï¼ˆç´¯ç§¯çš„Softmaxå’Œè¾“å‡ºï¼‰å†™å›DRAMã€‚è¿™æ ·å°±ä¸éœ€è¦ä¸€æ¬¡æ€§å­˜å‚¨æ•´ä¸ªå·¨å¤§çš„æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µã€‚

     - é¿å…å­˜å‚¨ä¸­é—´ç»“æœ: FlashAttention-2 å·§å¦™åœ°è®¾è®¡äº†å‰å‘å’Œåå‘ä¼ æ’­çš„è®¡ç®—æµï¼Œä½¿å¾—åå‘ä¼ æ’­æ—¶ï¼Œå¯ä»¥é‡æ–°è®¡ç®—è€Œä¸æ˜¯å­˜å‚¨ä¸­é—´ç»“æœã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒåœ¨åå‘ä¼ æ’­æ—¶ï¼Œä¼šå†æ¬¡ä½¿ç”¨ä¸å‰å‘ä¼ æ’­ç›¸åŒçš„åˆ‡ç‰‡æŠ€æœ¯ï¼Œé€å—è®¡ç®—æ¢¯åº¦ï¼Œè¿™æ ·å°±é¿å…äº†ä¿å­˜æ•´ä¸ª TÃ—T çŸ©é˜µã€‚


### problem
![alt text](image-10.png)
![alt text](image-11.png)
ç­”æ¡ˆæ˜¯`transformer_performance_analysis.html`
![alt text](image-12.png)


### problem
ä¸testsæ–‡ä»¶å¤¹ç›¸å…³çš„é—®é¢˜å…¨éƒ¨åœ¨testæ–‡ä»¶å¤¹ä¸‹è§£å†³äº†

### problem
![alt text](image-13.png)
ä»£ç åœ¨`assignment2-systems/cs336_systems/1.2/test_triton_speed.py`,ç»“æœåœ¨`assignment2-systems/test_triton.csv`


### problem
![alt text](image-14.png)
ä»£ç å’Œç»“æœåˆ†åˆ«æ˜¯`assignment2-systems/cs336_systems/2/benchmark_script.py`,`assignment2-systems/cs336_systems/2/benchmark_results.csv`


### problem
![alt text](image-15.png)
ä»£ç åœ¨`assignment2-systems/cs336_systems/2/naive_ddp.py`,ç»“æœä¸º
```bash
root@1ee5b610c063:/home/code_backup/code/cs336# python3 assignment2-systems/cs336_systems/2/naive_ddp.py
Starting naive DDP implementation test...
World size: 2, Epochs: 3, Batch size: 16, Learning rate: 0.01

Running simulated DDP training...
Epoch 1/3, Loss: 0.980953
Epoch 2/3, Loss: 0.975790
Epoch 3/3, Loss: 0.973346

Running single process training for comparison...
Running single process training for comparison
Epoch 1/3, Loss: 0.980953
Epoch 2/3, Loss: 0.975790
Epoch 3/3, Loss: 0.973346

Comparing model parameters:
âœ“ net.0.weight: Parameters match (max diff: 1.49e-08, mean diff: 4.45e-10)
âœ“ net.0.bias: Parameters match (max diff: 1.49e-08, mean diff: 1.16e-09)
âœ“ net.2.weight: Parameters match (max diff: 1.49e-08, mean diff: 2.51e-10)
âœ“ net.2.bias: Parameters match (max diff: 1.49e-08, mean diff: 1.43e-09)
âœ“ net.4.weight: Parameters match (max diff: 1.49e-08, mean diff: 2.36e-09)
âœ“ net.4.bias: Parameters match (max diff: 0.00e+00, mean diff: 0.00e+00)

Overall maximum difference: 1.49e-08
âœ… All parameters within tolerance!

âœ… SUCCESS: DDP implementation produces the same results as single-process training!
```


### problem
![alt text](image-16.png)
XLæ¨¡å‹å¤ªå¤§äº†ï¼Œæˆ‘è·‘ä¸èµ·æ¥ï¼Œæ‰€ä»¥æˆ‘è®¾ç½®äº†ä¸€ä¸ªæ¯”è¾ƒå°çš„æ¨¡å‹å‚æ•°
Hidden dim: 1024 â†’ 512 (å‡å°‘75%å†…å­˜)
Layers: 24 â†’ 12 (å‡å°‘50%å‚æ•°)
Attention heads: 16 â†’ 8
Sequence length: 1024 â†’ 512 (å‡å°‘75%æ¿€æ´»å†…å­˜)
Batch size: 2 â†’ 1 per GPU
Vocab size: 50257 â†’ 32000


ä»£ç åœ¨`assignment2-systems/cs336_systems/2/naive_ddp_benchmarking.py`
è¾“å‡ºçš„ç»“æœ
```bash
root@1ee5b610c063:/home/code_backup/code/cs336# python3 assignment2-systems/cs336_systems/2/naive_ddp_benchmarking.py
Starting Naive DDP Benchmarking (8GB GPU Optimized)
============================================================
This benchmark simulates distributed training communication overhead
while running on a single process to avoid environment issues.
Configuration has been optimized for 8GB GPU memory.
============================================================

Detected GPU: NVIDIA GeForce RTX 4060 Laptop GPU
GPU Memory: 8.6 GB
â„¹ï¸  GPU memory is limited. Using scaled-down model configuration.

Using device: cuda
GPU Memory: 8.6 GB
Creating Language Model (optimized for 8GB GPU)...
Model Configuration (Adapted for 8GB GPU):
  Parameters: 70,859,776
  Model size: ~0.28 GB
  Hidden dimension: 512 (scaled down from 1024)
  Layers: 12 (scaled down from 24)
  Attention heads: 8 (scaled down from 16)
  Sequence length: 512 (scaled down from 1024)
  Note: This is a smaller model to fit 8GB GPU memory
  The communication overhead patterns will be similar to XL model

Benchmarking Setup:
  World size: 2 (simulated)
  Batch size per GPU: 1
  Effective batch size: 2
  Benchmark steps: 10

Warming up...
Gradient checkpointing not available
Step  1/10: Total: 0.125s, Forward: 0.026s, Backward: 0.046s, Comm: 0.019s (15.4%), Loss: 10.4848
Step  2/10: Total: 0.122s, Forward: 0.024s, Backward: 0.045s, Comm: 0.019s (15.8%), Loss: 10.4405
Step  3/10: Total: 0.117s, Forward: 0.024s, Backward: 0.043s, Comm: 0.020s (17.3%), Loss: 10.5049
Step  4/10: Total: 0.126s, Forward: 0.023s, Backward: 0.043s, Comm: 0.030s (23.9%), Loss: 10.4757
Step  5/10: Total: 0.113s, Forward: 0.023s, Backward: 0.043s, Comm: 0.017s (15.3%), Loss: 10.4882
Step  6/10: Total: 0.113s, Forward: 0.024s, Backward: 0.043s, Comm: 0.017s (15.5%), Loss: 10.4688
Step  7/10: Total: 0.117s, Forward: 0.023s, Backward: 0.043s, Comm: 0.022s (18.3%), Loss: 10.4864
Step  8/10: Total: 0.122s, Forward: 0.024s, Backward: 0.043s, Comm: 0.025s (20.6%), Loss: 10.4681
Step  9/10: Total: 0.114s, Forward: 0.023s, Backward: 0.043s, Comm: 0.018s (16.1%), Loss: 10.4918
Step 10/10: Total: 0.121s, Forward: 0.024s, Backward: 0.043s, Comm: 0.024s (19.8%), Loss: 10.4755

================================================================================
NAIVE DDP BENCHMARKING RESULTS
================================================================================

Model Configuration:
  Model: XL (512d, 12 layers, 8 heads)
  Parameters: 70,859,776
  Model size: ~0.28 GB
  Sequence length: 512

Training Configuration:
  Setup: Single-node, 2 GPUs (simulated)
  Batch size per GPU: 1
  Effective batch size: 2
  Optimizer: AdamW

Timing Results (averaged over 10 steps):
  Total time per step:     0.1192 Â± 0.0045 seconds
    - Forward pass:        0.0239 Â± 0.0008 seconds (20.0%)
    - Backward pass:       0.0435 Â± 0.0010 seconds (36.5%)
    - Communication:       0.0213 Â± 0.0039 seconds (17.9%)
    - Optimizer step:      0.0266 Â± 0.0017 seconds (22.3%)

Communication Analysis:
  Average gradient data transferred: 283.4 MB per step
  Communication overhead: 17.9% of total step time
  âš ï¸  Moderate communication overhead

Performance Metrics:
  Throughput: 16.8 samples/second
  Tokens/second: 8593
  Estimated training time for 1M tokens: 0.0 hours

================================================================================
ANALYSIS & RECOMMENDATIONS
================================================================================
ğŸ” High Communication Overhead Detected:
   - Individual parameter all-reduce is inefficient
   - Consider gradient bucketing/fusion
   - Use optimized DDP implementations (e.g., PyTorch DDP)
   - Consider larger batch sizes to amortize communication cost

ğŸ“Š Detailed results saved to 'naive_ddp_benchmark_results.json'

================================================================================
SINGLE GPU COMPARISON BENCHMARK
================================================================================
Single GPU training with batch size 2...

Single GPU Results:
  Average step time: 0.0969 Â± 0.0013 seconds
  Forward pass:      0.0239 Â± 0.0006 seconds
  Backward pass:     0.0451 Â± 0.0006 seconds
  Optimizer step:    0.0272 Â± 0.0004 seconds
  Throughput:        20.6 samples/second

================================================================================
FINAL COMPARISON
================================================================================
DDP (simulated 2 GPUs):  16.8 samples/sec
Single GPU:              20.6 samples/sec
Speedup ratio:           0.81x
Communication overhead:  17.9%
âŒ Poor DDP scaling efficiency - high communication overhead!

ğŸ¯ Key Finding: Communication overhead is 17.9% of total training time
   This demonstrates the importance of optimizing gradient communication in DDP!

ğŸ“ Note: Results are from a scaled-down model due to 8GB GPU limitation.
   The communication overhead patterns would be similar for XL models.
   For the original XL model (1024d, 24L), communication overhead would likely be higher.
root@1ee5b610c063:/home/code_backup/code/cs336# 
```

**å¾—åˆ°çš„ç»“è®ºï¼šé€šä¿¡å¼€é”€æ˜¯æœ€å¤§çš„ç“¶é¢ˆã€‚åœ¨ä½ çš„â€œæœ´ç´ â€DDP å®ç°ä¸­ï¼Œé€šä¿¡ï¼ˆall-reduce æ¢¯åº¦ï¼‰å äº†æ€»è®­ç»ƒæ—¶é—´çš„ 17.9%ï¼Œè¿™æ˜¯ä¸€ä¸ªç›¸å½“é«˜çš„æ¯”ä¾‹ã€‚è®­ç»ƒæ•ˆç‡ä½ä¸‹ã€‚ç”±äºé«˜æ˜‚çš„é€šä¿¡æˆæœ¬ï¼Œæ¨¡æ‹Ÿ DDP çš„è®­ç»ƒé€Ÿåº¦ï¼ˆ16.8 æ ·æœ¬/ç§’ï¼‰åè€Œæ¯”å• GPU è®­ç»ƒï¼ˆ20.6 æ ·æœ¬/ç§’ï¼‰è¿˜è¦æ…¢ã€‚è¿™å¯¼è‡´äº† 0.81x çš„é€Ÿåº¦æ¯”ï¼Œè¡¨æ˜ DDP çš„æ€§èƒ½æ²¡æœ‰å¾—åˆ°æœ‰æ•ˆæå‡ï¼Œç”šè‡³è¿˜ä¸‹é™äº†ã€‚**


### problem
![alt text](image-17.png)
ä»£ç åœ¨`assignment2-systems/cs336_systems/2/minimal_ddp_flat_benchmarking.py`
ç»“æœ
```bash
root@1ee5b610c063:/home/code_backup/code/cs336# python3 assignment2-systems/cs336_systems/2/minimal_ddp_flat_benchmarking.py
DDP Gradient Batching Benchmark
==================================================
Comparing individual vs. batched gradient communication
This implements the improvement described in Â§2.3.1
==================================================

Using device: cuda
GPU Memory: 8.6 GB
Creating Language Model...
Model Configuration:
  Parameters: 70,859,776
  Model size: ~0.28 GB
  Configuration: 512d, 12L, 8H

Benchmarking Setup:
  World size: 2 (simulated)
  Batch size per GPU: 1
  Effective batch size: 2
  Benchmark steps: 12

================================================================================
BENCHMARKING INDIVIDUAL GRADIENT COMMUNICATION (Naive Approach)
================================================================================
Individual Step  1/12: Total: 0.1280s, Comm: 0.0329s (25.7%), Calls: 149, Loss: 10.4769
Individual Step  2/12: Total: 0.1274s, Comm: 0.0307s (24.1%), Calls: 149, Loss: 10.4878
Individual Step  3/12: Total: 0.1306s, Comm: 0.0289s (22.2%), Calls: 149, Loss: 10.4635
Individual Step  4/12: Total: 0.1204s, Comm: 0.0235s (19.6%), Calls: 149, Loss: 10.4650
Individual Step  5/12: Total: 0.1302s, Comm: 0.0337s (25.9%), Calls: 149, Loss: 10.4936
Individual Step  6/12: Total: 0.1281s, Comm: 0.0325s (25.3%), Calls: 149, Loss: 10.4889
Individual Step  7/12: Total: 0.1167s, Comm: 0.0212s (18.1%), Calls: 149, Loss: 10.4993
Individual Step  8/12: Total: 0.1296s, Comm: 0.0334s (25.8%), Calls: 149, Loss: 10.5067
Individual Step  9/12: Total: 0.1204s, Comm: 0.0239s (19.9%), Calls: 149, Loss: 10.4926
Individual Step 10/12: Total: 0.1219s, Comm: 0.0263s (21.6%), Calls: 149, Loss: 10.4901
Individual Step 11/12: Total: 0.1297s, Comm: 0.0338s (26.1%), Calls: 149, Loss: 10.4777
Individual Step 12/12: Total: 0.1302s, Comm: 0.0338s (26.0%), Calls: 149, Loss: 10.4910

================================================================================
BENCHMARKING BATCHED GRADIENT COMMUNICATION (Improved Approach)
================================================================================
Batched Step  1/12: Total: 0.1100s, Comm: 0.0147s (13.4%), Calls: 1, Loss: 10.4902
Batched Step  2/12: Total: 0.1120s, Comm: 0.0148s (13.2%), Calls: 1, Loss: 10.4789
Batched Step  3/12: Total: 0.1144s, Comm: 0.0148s (13.0%), Calls: 1, Loss: 10.4859
Batched Step  4/12: Total: 0.1122s, Comm: 0.0149s (13.3%), Calls: 1, Loss: 10.4909
Batched Step  5/12: Total: 0.1121s, Comm: 0.0150s (13.3%), Calls: 1, Loss: 10.4799
Batched Step  6/12: Total: 0.1109s, Comm: 0.0147s (13.3%), Calls: 1, Loss: 10.4748
Batched Step  7/12: Total: 0.1109s, Comm: 0.0148s (13.3%), Calls: 1, Loss: 10.4917
Batched Step  8/12: Total: 0.1106s, Comm: 0.0150s (13.6%), Calls: 1, Loss: 10.4679
Batched Step  9/12: Total: 0.1099s, Comm: 0.0148s (13.5%), Calls: 1, Loss: 10.4587
Batched Step 10/12: Total: 0.1112s, Comm: 0.0149s (13.4%), Calls: 1, Loss: 10.4904
Batched Step 11/12: Total: 0.1111s, Comm: 0.0151s (13.6%), Calls: 1, Loss: 10.5017
Batched Step 12/12: Total: 0.1119s, Comm: 0.0147s (13.2%), Calls: 1, Loss: 10.5052

================================================================================
COMPARISON RESULTS
================================================================================

Model Configuration:
  Parameters: 70,859,776
  Model size: ~0.28 GB
  World size: 2 GPUs
  Batch size per GPU: 1

Individual Gradient Communication (Naive Â§2.2):
  Average step time:        0.1261 Â± 0.0046 seconds
  Average communication:    0.0296 Â± 0.0045 seconds (23.4%)
  Communication calls:      149 per step

Batched Gradient Communication (Improved Â§2.3.1):
  Average step time:        0.1114 Â± 0.0012 seconds
  Average communication:    0.0149 Â± 0.0001 seconds (13.3%)
  Communication calls:      1 per step

============================================================
PERFORMANCE IMPROVEMENTS
============================================================
Step time improvement:        +11.6%
Communication time reduction: +49.7%
Throughput improvement:       +13.2%
Communication calls reduced:  149 â†’ 1 (-148)

Throughput Comparison:
  Individual approach: 15.9 samples/sec
  Batched approach:    17.9 samples/sec

============================================================
ANALYSIS
============================================================
âœ… Significant improvement from gradient batching!

ğŸ” Key Findings:
â€¢ Batching reduces communication calls from 149 to 1
â€¢ Communication overhead: 23.4% â†’ 13.3%
â€¢ The improvement demonstrates why modern DDP implementations use gradient bucketing

ğŸ“Š Detailed results saved to 'ddp_batching_comparison.json'

ğŸ¯ CONCLUSION:
Batching gradients into a single all-reduce call reduces communication
overhead by eliminating per-parameter startup costs, demonstrating why
modern DDP implementations use gradient bucketing strategies.
root@1ee5b610c063:/home/code_backup/code/cs336# 
```